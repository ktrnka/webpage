<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>MLOps Design Principles</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">MLOps Design Principles</h1>
</header>
<section data-field="subtitle" class="p-summary">
Design principles for deploying machine learning models that I’ve picked up over the years.
</section>
<section data-field="body" class="e-content">
<section name="0fdd" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="c81b" id="c81b" class="graf graf--h3 graf--leading graf--title">MLOps Design Principles</h3><p name="568a" id="568a" class="graf graf--p graf-after--h3">There’s a big difference between building a machine learning model that works on your computer, and making that model available for others to use. If implemented poorly, your users will be frustrated that your software isn’t reliable. And it can take months to implement it well!</p><p name="2c97" id="2c97" class="graf graf--p graf-after--p graf--trailing">In this article I’ll share the principles I’ve learned over the years in deploying models. It’s meant for readers with basic awareness of machine learning and software engineering.</p></div></div></section><section name="b06d" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="2be1" id="2be1" class="graf graf--h3 graf--leading">The basics</h3><p name="9dc7" id="9dc7" class="graf graf--p graf-after--h3">So you’ve got a model and want to integrate it into your product? It’s probably not a one-time thing! You’re going to need to fix bugs or release improvements! So you should think about having a process or system for updating models.</p><p name="ca04" id="ca04" class="graf graf--p graf-after--p">You’re going to want it to be easy for your team. You also want it to be reliable — your users won’t be happy if the service goes down when you update! Those are the two key concerns: <strong class="markup--strong markup--p-strong">minimizing defects and maximizing developer productivity</strong>.</p><p name="0fa5" id="0fa5" class="graf graf--p graf-after--p">I want to first acknowledge the bigger picture before getting into machine learning: These concerns aren’t specific to machine learning at all!</p><p name="2592" id="2592" class="graf graf--p graf-after--p">The DevOps movement is about breaking down harmful silos in organizations, and integrating the culture of writing software (dev) with culture of operating servers and networks (ops) to improve both productivity and reliability. I can’t summarize the entire field here, but these resources can help:</p><ul class="postList"><li name="5e79" id="5e79" class="graf graf--li graf-after--p"><a href="https://www.goodreads.com/book/show/17255186-the-phoenix-project" data-href="https://www.goodreads.com/book/show/17255186-the-phoenix-project" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">The Phoenix Project</a>: This is an entertaining read about a highly siloed, unproductive organization that improves.</li><li name="d8be" id="d8be" class="graf graf--li graf-after--li">The annual State of DevOps Report: These reports are full of gems. They tend to be hosted on a different website every year so you’ll have to search for it.</li><li name="5301" id="5301" class="graf graf--li graf-after--li"><a href="https://www.goodreads.com/en/book/show/35747076" data-href="https://www.goodreads.com/en/book/show/35747076" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Accelerate</a>: This is a book version derived from the annual DevOps reports and it’s well-worth a read.</li><li name="50b3" id="50b3" class="graf graf--li graf-after--li"><a href="https://sre.google/sre-book/table-of-contents/" data-href="https://sre.google/sre-book/table-of-contents/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">The Google Site Reliability Engineering (SRE) handbook</a>: This is another excellent resource for keeping your code running without downtime.</li><li name="64ed" id="64ed" class="graf graf--li graf-after--li">I’d also recommend looking through the links in this <a href="https://www.reddit.com/r/devops/comments/yjdscp/getting_into_devops/" data-href="https://www.reddit.com/r/devops/comments/yjdscp/getting_into_devops/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Reddit introduction</a>.</li></ul><p name="e8ca" id="e8ca" class="graf graf--p graf-after--li">Although the foundation of DevOps is cultural, in practice DevOps often refers to the processes, skills, and tools used to deliver software rapidly and reliably. Unfortunately many software delivery tools do not work well for the delivery of machine learning models. Machine learning also faces unique challenges, such as training and evaluating models.</p><p name="a2b8" id="a2b8" class="graf graf--p graf-after--p graf--trailing">MLOps is about building and delivering machine learning code+models rapidly and reliably. While MLOps inherits the cultural principles of DevOps, in practice it also addresses special considerations in processes, skills, and tools.</p></div></div></section><section name="0d73" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="268b" id="268b" class="graf graf--h3 graf--leading">MLOps Design Principles</h3><p name="2e84" id="2e84" class="graf graf--p graf-after--h3">These are lessons I’ve learned from deploying models at Swype, Nuance, and 98point6. I’ll explain the problems that motivated each principle and how I’ve applied it since.</p><p name="1dcd" id="1dcd" class="graf graf--p graf-after--p">I find it’s important to understand why you build software so that I can better adapt to new situations. That’s why I’m writing up principles — I’ve seen many people overly focus on tools without understanding the underlying reasons. And what’s more, each company comes with different challenges so the existing solutions may not apply, even if the principles do.</p><p name="83f3" id="83f3" class="graf graf--p graf-after--p">One last word before I get into it — I haven’t “solved” this field by any means. This reflects my current understanding from a decade of industry experience and I hope to improve my opinions in the future.</p><h4 name="a7ab" id="a7ab" class="graf graf--h4 graf-after--p">Version the code and model together, or link them</h4><p name="b21b" id="b21b" class="graf graf--p graf-after--h4">Your newest model may not be compatible with old versions of your code. So if something goes wrong and you need to revert, you need to know both the model and code version to revert to.</p><p name="0705" id="0705" class="graf graf--p graf-after--p">Likewise, a code change may require you to retrain your model. That can be anything from changing your neural network architecture to updating your dependencies to changing your preprocessing. In some cases, the new code will crash when trying to load a new model. In other cases, it might successfully load a model but have worse performance.</p><p name="00be" id="00be" class="graf graf--p graf-after--p">At Nuance we had an extreme version of code-model dependencies. We produced a software development kit (SDK) for phone makers (OEMs) to build their own keyboards. We also provided language databases which had word lists and language models in them. Manufacturers often wanted the latent language databases, but were wary of upgrading the SDK version. (*) So we maintained a giant compatibility table, listing the language database versions supported by all common versions of the SDK. In some cases, OEMs would use code from years prior and we couldn’t convince them to upgrade. So when we’d plan changes to our model files, we often had to discuss whether it could degrade reasonably in most prior versions of the code.</p><p name="dbc1" id="dbc1" class="graf graf--p graf-after--p">That experience motivated me to make code-model compatibility easier to track at 98point6. We used tools like <a href="https://dvc.org/" data-href="https://dvc.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">data version control (DVC)</a> which made it easy to track by storing models in S3 and linking model versions into git.</p><p name="105b" id="105b" class="graf graf--p graf-after--p">(*) OEMs often just didn’t have the time to update their integration for any breaking changes and often didn’t even have the time to validate an update. That said, most were willing to accept <a href="https://semver.org/" data-href="https://semver.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">patch versions</a>.</p><h4 name="02c3" id="02c3" class="graf graf--h4 graf-after--p">Atomic deploys and reverts</h4><p name="389f" id="389f" class="graf graf--p graf-after--h4">We don’t want downtime while deploying or reverting our software. And because the code and data versions must be aligned, it’s important to deploy or revert them as one unit. We don’t want a temporary outage when one of them has deployed but the other hasn’t yet.</p><p name="9819" id="9819" class="graf graf--p graf-after--p">This is easy if you package everything together, such as a Docker image or a mobile app. It’s more difficult if the model is stored outside of the code package. But it can be addressed by naming models with their version and explicitly loading the model by version in your code.</p><p name="7a62" id="7a62" class="graf graf--p graf-after--p">I only bring up this topic because I’ve heard of many people deploying models by manually overwriting an old model version, which doesn’t give us a nice way to deploy and revert.</p><p name="6edf" id="6edf" class="graf graf--p graf-after--p">Also note, there are similar DevOps challenges when deploying multiple components that must be kept in sync, such as a web service with a database schema change, or deploying a web service with infrastructure configuration changes. So if you’re looking to learn more you may find inspiration in the best practices for those kinds of deployments.</p><h4 name="a3c7" id="a3c7" class="graf graf--h4 graf-after--p">Training and serving in the same repo</h4><p name="7df3" id="7df3" class="graf graf--p graf-after--h4">At 98point6 we tried having separate repos and a single repo for machine learning projects and I found that development is faster and less error-prone with a single repo.</p><p name="fc36" id="fc36" class="graf graf--p graf-after--p">When there are training-serving interdependencies, separate repos can lead to this:</p><blockquote name="91d6" id="91d6" class="graf graf--blockquote graf-after--p">PR in training repo -&gt; merged -&gt; integrated into serving repo, only to find it doesn’t work -&gt; go back to the start</blockquote><p name="fee1" id="fee1" class="graf graf--p graf-after--blockquote">It’s an incredible waste of time that just doesn’t happen with a single repo.</p><p name="f3cb" id="f3cb" class="graf graf--p graf-after--p">One alternative would be to import code from the local copy of the training repo into the local copy of the serving repo. I’ve found that people tend to skip it and even when it’s done it’s sometime prone to copy-paste accidents.</p><p name="0cb7" id="0cb7" class="graf graf--p graf-after--p">The main downside of a single repo is that you need to implement multiple CI/CD pipelines in the same repo. That’s easy in some CI/CD platforms and harder in others.</p><p name="9410" id="9410" class="graf graf--p graf-after--p">I’ve found a similar lesson in general software engineering, for instance when a frontend and backend are split across repos, or multiple tightly coupled backends are split across repos. One warning sign you might have this is when people make PRs just to see if something works in another repo.</p><p name="d6b6" id="d6b6" class="graf graf--p graf-after--p">Demos are also closely related. If you have a minimal frontend demo in the repo, it allows developers to test more easily <em class="markup--em markup--p-em">before</em> anything goes to a pull request or deployment. At 98point6 we had some repos with demos in the repo and others with demos in a separate repo. It was much faster and less error-prone to develop in repos that had built-in demos.</p><h4 name="26b7" id="26b7" class="graf graf--h4 graf-after--p">Single implementation of models</h4><p name="ab52" id="ab52" class="graf graf--p graf-after--h4">It’s best to have a single implementation of the model that’s used in both training and serving.</p><p name="c459" id="c459" class="graf graf--p graf-after--p">At Nuance we had a separate implementation in training in Python and production in C. The Python implementation trained the model and evaluated it on held-out data. The C implementation loaded the model and had some basic unit tests. Over the years, we discovered that the two implementations didn’t handle some cases the same way (like the start of a sentence) and that meant that the production models were suffering degraded quality.</p><p name="bd96" id="bd96" class="graf graf--p graf-after--p">There are many ways to address this. TensorFlow Lite and PyTorch Mobile are for mobile apps. TensorFlow.js is for Javascript. And ONNX is a cross-platform, cross-framework file format with runtimes for many languages. That said, as I write this I’m learning that it <a href="https://news.ycombinator.com/item?id=34799597" data-href="https://news.ycombinator.com/item?id=34799597" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">can be difficult to make your models compatible with ONNX</a>.</p><p name="eb1f" id="eb1f" class="graf graf--p graf-after--p">If you can’t share the model code, you should at least test that the two implementations behave similarly.</p><h4 name="1cbd" id="1cbd" class="graf graf--h4 graf-after--p">Review and approve your model releases</h4><p name="2370" id="2370" class="graf graf--p graf-after--h4">It’s <em class="markup--em markup--p-em">very tempting</em> to only deploy models with better evaluations. That’s a subtly flawed approach because your testing data may change. If your testing data changes, you don’t know whether the numbers should go up or down. You might fix a bug in the testing data only to see that the metric was overly optimistic and now looks worse. Or you may update your testing data periodically and find that the distribution has shifted over time, which could make it easier or harder to get good metrics.</p><p name="84bf" id="84bf" class="graf graf--p graf-after--p">A human should review the evaluation report. If anything looks odd, they should review the model outputs on some standardized examples to spot anything that might not be detected in automated evaluation. Ideally you should have guidelines that the team can follow when doing this, and they can use their best judgement with your guidelines. If you use Github, I’d recommend the pull request process for this.</p><p name="59b1" id="59b1" class="graf graf--p graf-after--p">I learned this the hard way at Nuance. We periodically found quality issues in our testing data, such as the way it was preprocessed for Unicode, how duplicates were removed, or how language identification worked. It was particularly common for new languages with new data.</p><p name="650d" id="650d" class="graf graf--p graf-after--p">At 98point6 our model builds produced a training report and opened a pull request. If anything looked odd we could investigate it manually. This gave us the flexibility to change our testing data and the process to review our models. We improved it over time by developing guidelines for the team and putting them in the repo’s pull request template.</p><h4 name="f8cd" id="f8cd" class="graf graf--h4 graf-after--p">Test, not too much, not too little</h4><p name="446f" id="446f" class="graf graf--p graf-after--h4">Testing is a complex topic that could take a whole article. If you haven’t read it, I strongly recommend reading <a href="http://www.agitar.com/downloads/TheWayOfTestivus.pdf" data-href="http://www.agitar.com/downloads/TheWayOfTestivus.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Testivus</a> first because that’s my stance.</p><p name="efb5" id="efb5" class="graf graf--p graf-after--p">First off, I want to acknowledge that evaluating your model on held-out data <em class="markup--em markup--p-em">is</em> testing. It’s often ignored when talking about testing but it’s a key component of quality control for machine learning.</p><p name="d374" id="d374" class="graf graf--p graf-after--p">The challenge I experienced at Nuance and 98point6 was that we wanted unit tests that depend on the models, but the models change often. For example, the word predictions for “I am ___” might change when we improve our model. If we have unit tests that depend on those predictions, they will fail and someone will think they’ve broken the code when they haven’t. If those tests are maintained, they’ll be frequently edited and don’t offer much value as tests.</p><p name="eef3" id="eef3" class="graf graf--p graf-after--p">So on to my guideline: Test that the model loads. Don’t extensively unit test the model’s behavior. You already tested the specific outputs against held-out data in a way that’s much more robust than unit testing anyway. We adopted that stance at 98point6 and it was an improvement.</p><p name="72d0" id="72d0" class="graf graf--p graf-after--p">Side note: Design your code base so that you can unit test any business logic independently from your machine learning model. If you do that, the business logic tests will be less flakey, and they’ll run faster too.</p><h3 name="e2ab" id="e2ab" class="graf graf--h3 graf-after--p">Automate when possible</h3><p name="3562" id="3562" class="graf graf--p graf-after--h3">Reduce the burden on your devs by automating what you can, such as retraining models with new data or deploying a model once it’s validated. Automate testing when you can.</p><p name="b524" id="b524" class="graf graf--p graf-after--p">Be cautious about automating too soon — automation isn’t always worth the effort, and it’s also risky to automate a process before really understanding the challenges.</p><p name="65fa" id="65fa" class="graf graf--p graf-after--p">In cases where testing wasn’t automated, I found that some devs would forget to run tests and push buggy code or models. Then it would disrupt another developer, who would think they broke something.</p><p name="2a50" id="2a50" class="graf graf--p graf-after--p">In cases where deployment wasn’t automated, non-experts were sometimes afraid they’d break something and would wait for “the expert” to be available. That could lead to delays and moreover the culture felt unhealthy. It’s better if everyone can deploy and only occassional issues need a specialist.</p><p name="5639" id="5639" class="graf graf--p graf-after--p">In cases where training wasn’t automated and wasn’t trivial, I found that one person would become “the training person”. That puts an unfair burden on one person and makes the team less resilient, not to mention any delays if they take a vacation.</p><h3 name="d80c" id="d80c" class="graf graf--h3 graf-after--p">Early ideas</h3><p name="1df7" id="1df7" class="graf graf--p graf-after--h3">These are a couple of design principles I’m entertaining, but not yet certain of.</p><h4 name="bc59" id="bc59" class="graf graf--h4 graf-after--p">Snapshotting dependencies</h4><p name="ed3a" id="ed3a" class="graf graf--p graf-after--h4">I prefer to leave dependencies unpinned when possible so that they’re updated by frequent rebuilds. That way, we’re getting minor patches pretty frequently. And if there are breaking changes we’re exposed to them a little at a time rather than all at once in a big update.</p><p name="b13a" id="b13a" class="graf graf--p graf-after--p">I came to prefer this style of dependencies after pinning dependencies in repos at 98point6 and finding that it typically led to very outdated versions of libraries. Then when a crisis happened, it forced an upgrade of many dependencies all at once which led to delays in a crisis situation.</p><p name="1536" id="1536" class="graf graf--p graf-after--p">This sort of thing may vary by company, for instance if you have tooling to periodically test your code with updated dependencies, or if you have a strong culture around dependency updates.</p><p name="4763" id="4763" class="graf graf--p graf-after--p">In the context of machine learning, the model artifact may depend on certain installed libraries. Scikit-learn models can be sensitive to the version of scikit-learn and joblib, for example. They may just not load in a different version. So the version of those dependencies needs to be pinned in the serving code. You just can’t take a chance with another version. And if you have automated testing of new version, it’ll just fail unless you code it to retrain on certain dependency updates.</p><p name="6592" id="6592" class="graf graf--p graf-after--p">I also want to mention that saving your models with ONNX and other library-independent formats may fix this problem altogether for you, so long as you don’t also have any preprocessing code that needs to be released to the service.</p><h4 name="99ea" id="99ea" class="graf graf--h4 graf-after--p">Use dependencies, not too many, not too few</h4><p name="2982" id="2982" class="graf graf--p graf-after--h4">It’s generally good to use well-tested, publicly-available code instead of writing new code. For example, using PyTorch will be better than writing your own neural network library from scratch because it’s highly tested and optimized.</p><p name="8bd6" id="8bd6" class="graf graf--p graf-after--p">On the other hand, I’ve seen code that goes overboard with dependencies that aren’t strictly needed. Unfortunately, each dependency is an opportunity for a performance bug or security vulnerability. I found this was a problem in the Javascript ecosystem. As a result, projects which depended on Javascript would often trigger security scan warnings. At 98point6 I experienced this because of CDK in Javascript — we’d go to deploy a minor update to an old service only to find that the Javascript dependencies of CDK had tripped the security scanner. Then we’d have to go figure out the update process.</p><p name="c0f1" id="c0f1" class="graf graf--p graf-after--p">In contrast, Swype and Nuance were on the extreme minimal side of dependencies. In my first two months I learned that we couldn’t use standard template library (STL) in C++ because some of our customers required compilers without STL. So instead I wrote my own data structures which were certainly less tested, documented, and performant than STL. If I could do it again I’d explore other options.</p><p name="8568" id="8568" class="graf graf--p graf-after--p">There’s another cost to dependencies — it’s more for new developers to learn. When you’re adding more dependencies and complexity, it’s good to double-check that the benefit will be worthwhile even for new teammates.</p><h3 name="2e18" id="2e18" class="graf graf--h3 graf-after--p">What’s next?</h3><p name="4c68" id="4c68" class="graf graf--p graf-after--h3">Thanks for reading!</p><p name="8354" id="8354" class="graf graf--p graf-after--p graf--trailing"><a href="https://medium.com/@keith.trnka/mlops-repo-walkthrough-90c7bd275f53" data-href="https://medium.com/@keith.trnka/mlops-repo-walkthrough-90c7bd275f53" class="markup--anchor markup--p-anchor" target="_blank">In the next post I walk through an example repo that shows MLOps for web services on AWS</a>. After explaining the repo I’ll also audit it.</p></div></div></section><section name="2dc8" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="0abd" id="0abd" class="graf graf--h3 graf--leading">Appendix: Types of deployments</h3><p name="c841" id="c841" class="graf graf--p graf-after--h3">There are many different kinds of deployments and they affect MLOps differently. I’ll provide a brief summary of how they differ here:</p><ul class="postList"><li name="453e" id="453e" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Web services</strong>: Latency is a major factor to consider, and that leads to things like auto-scaling or geographic distribution. And don’t forget that you’re paying for that compute too!</li><li name="bc7c" id="bc7c" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Code packages</strong>: Releasing your model and code as an importable package frees you from thinking so much about the compute environment. But your models may be integrated much slower than you’d like. Also, you may not get crash reporting.</li><li name="9126" id="9126" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Mobile apps</strong>: Mobile devices vary widely in terms of compute and memory, which affects the kinds of models you can ship. You also don’t have precise control over getting your update to everyone’s phone, especially on iOS.</li><li name="0700" id="0700" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Analytics data warehouse / ETL</strong>: If you update a model, should you re-run predictions on all old data? Or just new? How will you communicate that to your users? Software dependencies can be a challenge depending on your platform too.</li><li name="6bc7" id="6bc7" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Web frontends</strong>: The available compute and memory varies widely across users. You also don’t have full control over when they refresh and get updates by default.</li></ul><p name="c324" id="c324" class="graf graf--p graf-after--li graf--trailing">I’ve worked with most of these except deploying models into web frontends. I’m sure there are other types of deployments with unique challenges as well.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@keith.trnka" class="p-author h-card">Keith Trnka</a> on <a href="https://medium.com/p/e30cc40442a1"><time class="dt-published" datetime="2023-02-16T00:32:37.802Z">February 16, 2023</time></a>.</p><p><a href="https://medium.com/@keith.trnka/mlops-design-principles-e30cc40442a1" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on February 18, 2026.</p></footer></article></body></html>